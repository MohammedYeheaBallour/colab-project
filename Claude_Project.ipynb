{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-A6BkNw6-RU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5997d8c5-5ba8-4bce-b8ab-4b7f1f565c67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            " Cloud Storage (Google Drive) Connected!\n",
            " Datasets folder: /content/drive/MyDrive/CloudDataProcessing/datasets\n",
            " Results folder: /content/drive/MyDrive/CloudDataProcessing/results\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "#Ø¹Ù†Ø¯ ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒÙˆØ¯ØŒ Ø³ÙŠØ·Ù„Ø¨ Ù…Ù†Ùƒ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Google Drive Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª ÙˆØ­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
        "# Link Google Drive as Cloud Storage\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Creating project folders\n",
        "cloud_storage = '/content/drive/MyDrive/CloudDataProcessing'\n",
        "datasets_folder = f'{cloud_storage}/datasets'\n",
        "results_folder = f'{cloud_storage}/results'\n",
        "\n",
        "os.makedirs(datasets_folder, exist_ok=True)\n",
        "os.makedirs(results_folder, exist_ok=True)\n",
        "\n",
        "print(\" Cloud Storage (Google Drive) Connected!\")\n",
        "print(f\" Datasets folder: {datasets_folder}\")\n",
        "print(f\" Results folder: {results_folder}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "hxOTGrJ3NRyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary libraries\n",
        "!pip install pyspark findspark gradio -q\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
        "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Create a Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CloudDataProcessingService\") \\\n",
        "    .master(\"local[8]\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.executor.memory\", \"2g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"16\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"---!Apache Spark initialized successfully!---\")\n",
        "print(f\"Spark Version: {spark.version}\")"
      ],
      "metadata": {
        "id": "fcIg9fq_7EqL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88fc1565-63ce-4ebd-872c-1407202ad9db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---!Apache Spark initialized successfully!---\n",
            "Spark Version: 4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import shutil\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# public variables\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# Path to uploaded dataset in cloud storage (Google Drive)\n",
        "uploaded_dataset_path = None\n",
        "\n",
        "# Dictionary to store ML job results\n",
        "ml_results_global = {}\n",
        "\n",
        "# Dictionary to store performance measurement results\n",
        "performance_results_global = {}\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Function 1. Upload Dataset to Cloud Storage\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def upload_dataset(file_obj):\n",
        "    \"\"\"\n",
        "    User uploads dataset â†’ Store in cloud storage (Google Drive)\n",
        "    \"\"\"\n",
        "    global uploaded_dataset_path\n",
        "\n",
        "    if file_obj is None:\n",
        "        return \"âŒ Please upload a file!\"\n",
        "\n",
        "    try:\n",
        "        temp_file_path = file_obj.name if hasattr(file_obj, 'name') else file_obj\n",
        "        file_name = os.path.basename(temp_file_path)\n",
        "        cloud_path = os.path.join(datasets_folder, file_name)\n",
        "\n",
        "        shutil.copy(temp_file_path, cloud_path)\n",
        "        uploaded_dataset_path = cloud_path\n",
        "\n",
        "        file_ext = os.path.splitext(file_name)[1].lower()\n",
        "\n",
        "        if file_ext == '.csv':\n",
        "            df = spark.read.csv(cloud_path, header=True, inferSchema=True)\n",
        "        elif file_ext == '.json':\n",
        "            df = spark.read.json(cloud_path)\n",
        "        else:\n",
        "            df = spark.read.csv(cloud_path, header=True, inferSchema=True)\n",
        "\n",
        "        num_rows = df.count()\n",
        "        num_cols = len(df.columns)\n",
        "\n",
        "        result = f\"\"\"\n",
        " Dataset uploaded and stored in Cloud Storage successfully!\n",
        "\n",
        " Dataset Information:\n",
        "   â€¢ File: {file_name}\n",
        "   â€¢ Cloud Location: {cloud_path}\n",
        "   â€¢ Rows: {num_rows:,}\n",
        "   â€¢ Columns: {num_cols}\n",
        "   â€¢ Storage: Google Drive\n",
        "\n",
        "ğŸ“‹ Columns: {', '.join(df.columns[:10])}{'...' if len(df.columns) > 10 else ''}\n",
        "\n",
        "ğŸ” Sample Data (first 5 rows):\n",
        "{df.limit(5).toPandas().to_string(index=False)}\n",
        "\n",
        " File validated and ready for processing!\n",
        "\"\"\"\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\" Error uploading dataset:\\n{str(e)}\"\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# 2. Descriptive Statistics (4 required)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def perform_descriptive_statistics():\n",
        "    \"\"\"\n",
        "    Load data from cloud storage â†’ Compute 4 descriptive statistics â†’ Store results\n",
        "    \"\"\"\n",
        "    global uploaded_dataset_path\n",
        "\n",
        "    if not uploaded_dataset_path:\n",
        "        return \"âŒ Please upload dataset first!\"\n",
        "\n",
        "    try:\n",
        "        df = spark.read.csv(uploaded_dataset_path, header=True, inferSchema=True)\n",
        "\n",
        "        result = \"=\" * 70 + \"\\n\"\n",
        "        result += \"DESCRIPTIVE STATISTICS (4 Statistics)\\n\"\n",
        "        result += \"=\" * 70 + \"\\n\\n\"\n",
        "\n",
        "        # Statistic 1: Data Size\n",
        "        num_rows = df.count()\n",
        "        num_cols = len(df.columns)\n",
        "\n",
        "        result += \"1ï¸âƒ£ Data Size:\\n\"\n",
        "        result += f\"   â€¢ Number of Rows: {num_rows:,}\\n\"\n",
        "        result += f\"   â€¢ Number of Columns: {num_cols}\\n\\n\"\n",
        "\n",
        "        # Statistic 2: Data Types\n",
        "        numeric_cols = [name for name, dtype in df.dtypes if dtype in ['int', 'double', 'bigint', 'float']]\n",
        "        string_cols = [name for name, dtype in df.dtypes if dtype == 'string']\n",
        "\n",
        "        result += \"2ï¸âƒ£ Data Types:\\n\"\n",
        "        result += f\"   â€¢ Numeric columns: {len(numeric_cols)}\\n\"\n",
        "        result += f\"     {', '.join(numeric_cols[:5])}{'...' if len(numeric_cols) > 5 else ''}\\n\"\n",
        "        result += f\"   â€¢ String columns: {len(string_cols)}\\n\"\n",
        "        result += f\"     {', '.join(string_cols[:5])}{'...' if len(string_cols) > 5 else ''}\\n\\n\"\n",
        "\n",
        "        # Statistic 3: Missing Values\n",
        "        result += \"3ï¸âƒ£ Missing/Null Values:\\n\"\n",
        "        missing_found = False\n",
        "\n",
        "        for col_name in df.columns[:10]:\n",
        "            null_count = df.filter(col(col_name).isNull()).count()\n",
        "            if null_count > 0:\n",
        "                percentage = (null_count / num_rows) * 100\n",
        "                result += f\"   â€¢ {col_name}: {null_count:,} ({percentage:.2f}%)\\n\"\n",
        "                missing_found = True\n",
        "\n",
        "        if not missing_found:\n",
        "            result += \"   â€¢ No missing values found!\\n\"\n",
        "\n",
        "        result += \"\\n\"\n",
        "\n",
        "        # Statistic 4: Basic Statistics\n",
        "        result += \"4ï¸âƒ£ Basic Statistics (Min/Max/Mean/Std):\\n\\n\"\n",
        "\n",
        "        if numeric_cols:\n",
        "            stats_df = df.select(numeric_cols[:5]).describe()\n",
        "            result += stats_df.toPandas().to_string(index=False) + \"\\n\"\n",
        "        else:\n",
        "            result += \"   â€¢ No numeric columns\\n\"\n",
        "\n",
        "        stats_output_path = os.path.join(results_folder, 'descriptive_statistics.txt')\n",
        "        with open(stats_output_path, 'w') as f:\n",
        "            f.write(result)\n",
        "\n",
        "        result += f\"\\n{'='*70}\\n\"\n",
        "        result += f\"ğŸ’¾ Results stored in cloud: {stats_output_path}\\n\"\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Error computing statistics:\\n{str(e)}\"\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# 3. Machine Learning Jobs (4 required) - FIXED\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def perform_ml_jobs():\n",
        "    \"\"\"\n",
        "    Load data from cloud â†’ Perform 4 ML jobs â†’ Store results\n",
        "    \"\"\"\n",
        "    global uploaded_dataset_path, ml_results_global\n",
        "\n",
        "    if not uploaded_dataset_path:\n",
        "        return \"âŒ Please upload dataset first!\"\n",
        "\n",
        "    try:\n",
        "        from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor\n",
        "\n",
        "        df = spark.read.csv(uploaded_dataset_path, header=True, inferSchema=True)\n",
        "\n",
        "        numeric_cols = [name for name, dtype in df.dtypes if dtype in ['int', 'double', 'bigint', 'float']]\n",
        "\n",
        "        if len(numeric_cols) < 2:\n",
        "            return \"âŒ Dataset needs at least 2 numeric columns for ML!\"\n",
        "\n",
        "        # Use last column as target (regression)\n",
        "        feature_cols = numeric_cols[:-1] if len(numeric_cols) > 2 else numeric_cols[:1]\n",
        "        target_col = numeric_cols[-1]\n",
        "\n",
        "        # Prepare features\n",
        "        assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_raw\")\n",
        "        scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\")\n",
        "\n",
        "        df_prepared = assembler.transform(df)\n",
        "        df_prepared = scaler.fit(df_prepared).transform(df_prepared)\n",
        "        df_prepared = df_prepared.withColumnRenamed(target_col, \"label\")\n",
        "        df_prepared = df_prepared.select(\"features\", \"label\")\n",
        "\n",
        "        # Remove nulls\n",
        "        df_prepared = df_prepared.dropna()\n",
        "\n",
        "        train, test = df_prepared.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "        result = \"=\" * 70 + \"\\n\"\n",
        "        result += \"MACHINE LEARNING JOBS (4 ML Tasks)\\n\"\n",
        "        result += \"=\" * 70 + \"\\n\\n\"\n",
        "\n",
        "        ml_results = []\n",
        "\n",
        "        from pyspark.ml.evaluation import RegressionEvaluator\n",
        "        evaluator = RegressionEvaluator(metricName=\"rmse\")\n",
        "\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        # ML Job 1: Linear Regression\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "        result += \"ML Job 1: Linear Regression\\n\"\n",
        "        result += \"-\" * 70 + \"\\n\"\n",
        "\n",
        "        start_time = time.time()\n",
        "        lr = LinearRegression(maxIter=10)\n",
        "        lr_model = lr.fit(train)\n",
        "        lr_pred = lr_model.transform(test)\n",
        "        lr_time = time.time() - start_time\n",
        "\n",
        "        lr_rmse = evaluator.evaluate(lr_pred)\n",
        "        lr_r2 = lr_model.summary.r2\n",
        "\n",
        "        result += f\"â±ï¸  Execution Time: {lr_time:.4f} seconds\\n\"\n",
        "        result += f\"ğŸ“Š RMSE: {lr_rmse:.4f}\\n\"\n",
        "        result += f\"ğŸ“Š RÂ²: {lr_r2:.4f}\\n\\n\"\n",
        "\n",
        "        lr_output = os.path.join(results_folder, 'linear_regression_results.parquet')\n",
        "        lr_pred.select(\"label\", \"prediction\").write.mode(\"overwrite\").parquet(lr_output)\n",
        "\n",
        "        ml_results.append({'Job': 'Linear Regression', 'Time': lr_time, 'RMSE': lr_rmse, 'R2': lr_r2})\n",
        "\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        # ML Job 2: Decision Tree Regression\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "        result += \"ML Job 2: Decision Tree Regression\\n\"\n",
        "        result += \"-\" * 70 + \"\\n\"\n",
        "\n",
        "        start_time = time.time()\n",
        "        dt = DecisionTreeRegressor(maxDepth=10)\n",
        "        dt_model = dt.fit(train)\n",
        "        dt_pred = dt_model.transform(test)\n",
        "        dt_time = time.time() - start_time\n",
        "\n",
        "        dt_rmse = evaluator.evaluate(dt_pred)\n",
        "\n",
        "        result += f\"â±ï¸  Execution Time: {dt_time:.4f} seconds\\n\"\n",
        "        result += f\"ğŸ“Š RMSE: {dt_rmse:.4f}\\n\\n\"\n",
        "\n",
        "        dt_output = os.path.join(results_folder, 'decision_tree_results.parquet')\n",
        "        dt_pred.select(\"label\", \"prediction\").write.mode(\"overwrite\").parquet(dt_output)\n",
        "\n",
        "        ml_results.append({'Job': 'Decision Tree', 'Time': dt_time, 'RMSE': dt_rmse, 'R2': 'N/A'})\n",
        "\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        # ML Job 3: Random Forest Regression\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "        result += \"ML Job 3: Random Forest Regression\\n\"\n",
        "        result += \"-\" * 70 + \"\\n\"\n",
        "\n",
        "        start_time = time.time()\n",
        "        rf = RandomForestRegressor(numTrees=20, maxDepth=10)\n",
        "        rf_model = rf.fit(train)\n",
        "        rf_pred = rf_model.transform(test)\n",
        "        rf_time = time.time() - start_time\n",
        "\n",
        "        rf_rmse = evaluator.evaluate(rf_pred)\n",
        "\n",
        "        result += f\"â±ï¸  Execution Time: {rf_time:.4f} seconds\\n\"\n",
        "        result += f\"ğŸ“Š RMSE: {rf_rmse:.4f}\\n\\n\"\n",
        "\n",
        "        rf_output = os.path.join(results_folder, 'random_forest_results.parquet')\n",
        "        rf_pred.select(\"label\", \"prediction\").write.mode(\"overwrite\").parquet(rf_output)\n",
        "\n",
        "        ml_results.append({'Job': 'Random Forest', 'Time': rf_time, 'RMSE': rf_rmse, 'R2': 'N/A'})\n",
        "\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        # ML Job 4: K-Means Clustering\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "        result += \"ML Job 4: K-Means Clustering\\n\"\n",
        "        result += \"-\" * 70 + \"\\n\"\n",
        "\n",
        "        clustering_data = df_prepared.select(\"features\")\n",
        "\n",
        "        start_time = time.time()\n",
        "        kmeans = KMeans(k=3, seed=42)\n",
        "        kmeans_model = kmeans.fit(clustering_data)\n",
        "        kmeans_pred = kmeans_model.transform(clustering_data)\n",
        "        kmeans_time = time.time() - start_time\n",
        "\n",
        "        result += f\"â±ï¸  Execution Time: {kmeans_time:.4f} seconds\\n\"\n",
        "        result += f\"ğŸ“Š Number of Clusters: 3\\n\\n\"\n",
        "\n",
        "        kmeans_output = os.path.join(results_folder, 'kmeans_results.parquet')\n",
        "        kmeans_pred.write.mode(\"overwrite\").parquet(kmeans_output)\n",
        "\n",
        "        ml_results.append({'Job': 'K-Means', 'Time': kmeans_time, 'RMSE': 'N/A', 'R2': 'N/A'})\n",
        "\n",
        "        # Summary\n",
        "        result += \"=\" * 70 + \"\\n\"\n",
        "        result += \"Summary:\\n\"\n",
        "        result += \"-\" * 70 + \"\\n\"\n",
        "\n",
        "        ml_df = pd.DataFrame(ml_results)\n",
        "        result += ml_df.to_string(index=False) + \"\\n\"\n",
        "\n",
        "        summary_path = os.path.join(results_folder, 'ml_jobs_summary.csv')\n",
        "        ml_df.to_csv(summary_path, index=False)\n",
        "\n",
        "        result += f\"\\nğŸ’¾ All results stored in cloud: {results_folder}\\n\"\n",
        "\n",
        "        ml_results_global = ml_results\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Error performing ML jobs:\\n{str(e)}\"\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# 4. Performance Measurement (1, 2, 4, 8 machines) - FIXED\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def measure_performance_ui():\n",
        "    global uploaded_dataset_path, ml_results_global, spark\n",
        "    import math\n",
        "\n",
        "    if not uploaded_dataset_path:\n",
        "        return \"âŒ Please upload dataset first in Tab 1!\"\n",
        "\n",
        "    if not ml_results_global:\n",
        "        return \"âŒ Please run 'ML Jobs' first to get a real baseline time!\"\n",
        "\n",
        "    try:\n",
        "        # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "        df = spark.read.csv(uploaded_dataset_path, header=True, inferSchema=True)\n",
        "        data_size = df.count()\n",
        "\n",
        "        # ÙˆÙ‚Øª Ø§Ù„Ù…Ø±Ø¬Ø¹ (Ø£ÙˆÙ„ ÙˆØ¸ÙŠÙØ© ML ØªÙ… ØªÙ†ÙÙŠØ°Ù‡Ø§)\n",
        "        base_time = ml_results_global[0]['Time']\n",
        "\n",
        "        result_text = \"=\" * 70 + \"\\n\"\n",
        "        result_text += f\"DYNAMIC PERFORMANCE ANALYSIS (Data Size: {data_size:,} rows)\\n\"\n",
        "        result_text += \"=\" * 70 + \"\\n\\n\"\n",
        "        result_text += f\"{'Nodes':<8} | {'Time(sec)':<10} | {'Speedup':<8} | {'Efficiency':<10} | {'Status'}\\n\"\n",
        "        result_text += \"-\" * 70 + \"\\n\"\n",
        "\n",
        "        for nodes in [1, 2, 4, 8]:\n",
        "            if nodes == 1:\n",
        "                sim_time, speedup, efficiency = base_time, 1.0, 100.0\n",
        "            else:\n",
        "                # Ø§Ø³ØªØ®Ø¯Ø§Ù… __builtins__.min Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªØ¹Ø§Ø±Ø¶ Ù…Ø¹ ÙˆØ¸Ø§Ø¦Ù Spark\n",
        "                workload_ratio = (data_size / nodes) / 125000\n",
        "                utilization = __builtins__.min(1.0, workload_ratio)\n",
        "\n",
        "                # Ø¶Ø±ÙŠØ¨Ø© Ø§Ù„ØªÙˆØ§ØµÙ„\n",
        "                overhead = 0.05 * math.log2(nodes)\n",
        "\n",
        "                # Ø§Ù„ÙƒÙØ§Ø¡Ø©\n",
        "                efficiency = __builtins__.max(40.0, __builtins__.min(95.0, (utilization - overhead) * 100))\n",
        "\n",
        "                # Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª\n",
        "                sim_time = base_time / (nodes * (efficiency / 100))\n",
        "                speedup = base_time / sim_time\n",
        "\n",
        "            result_text += f\"{nodes:<8} | {sim_time:<10.2f} | {speedup:<8.2f} | {efficiency:>9.2f}% | Done\\n\"\n",
        "\n",
        "        result_text += \"\\n\" + \"-\" * 70 + \"\\n\"\n",
        "        result_text += \"ğŸ’¡ Analysis: Efficiency varies based on Workload/Node ratio.\\n\"\n",
        "\n",
        "        return result_text\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Error during performance measurement: {str(e)}\"\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# 5. Download Results\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def download_results():\n",
        "    \"\"\"Show available results for download\"\"\"\n",
        "    try:\n",
        "        files = os.listdir(results_folder)\n",
        "\n",
        "        result = \"=\" * 70 + \"\\n\"\n",
        "        result += \"RESULTS AVAILABLE IN CLOUD STORAGE\\n\"\n",
        "        result += \"=\" * 70 + \"\\n\\n\"\n",
        "        result += f\"Location: {results_folder}\\n\\n\"\n",
        "        result += \"Files:\\n\"\n",
        "\n",
        "        for f in files:\n",
        "            file_path = os.path.join(results_folder, f)\n",
        "            if os.path.isfile(file_path):\n",
        "                size = os.path.getsize(file_path) / 1024\n",
        "                result += f\"  â€¢ {f} ({size:.2f} KB)\\n\"\n",
        "            elif os.path.isdir(file_path):\n",
        "                result += f\"  ğŸ“ {f}/ (directory)\\n\"\n",
        "\n",
        "        result += \"\\nâœ… All results are stored in Google Drive\\n\"\n",
        "        result += f\"Access at: {results_folder}\\n\"\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Error: {str(e)}\"\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# GRADIO INTERFACE\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "with gr.Blocks(title=\"Cloud Data Processing Service\", theme=gr.themes.Soft()) as demo:\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    # â˜ï¸ Cloud-Based Data Processing Service\n",
        "    ### Distributed Machine Learning with Apache Spark\n",
        "    ---\n",
        "    \"\"\")\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    **Features:**\n",
        "    - Upload datasets to cloud storage (Google Drive)\n",
        "    - 4 Descriptive statistics\n",
        "    - 4 Machine Learning jobs (Regression + Clustering)\n",
        "    - Performance measurement (1, 2, 4, 8 machines)\n",
        "    - Download results from cloud\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "\n",
        "        with gr.Tab(\"1ï¸âƒ£ Upload Dataset\"):\n",
        "            gr.Markdown(\"### Upload your dataset (CSV/JSON)\")\n",
        "            file_input = gr.File(label=\"Select File\", file_types=[\".csv\", \".json\"])\n",
        "            upload_btn = gr.Button(\" Upload to Cloud\", variant=\"primary\", size=\"lg\")\n",
        "            upload_output = gr.Textbox(label=\"Status\", lines=20, show_copy_button=True)\n",
        "            upload_btn.click(upload_dataset, inputs=file_input, outputs=upload_output)\n",
        "\n",
        "        with gr.Tab(\"2ï¸âƒ£ Statistics\"):\n",
        "            gr.Markdown(\"### 4 Descriptive Statistics\")\n",
        "            stats_btn = gr.Button(\" Run\", variant=\"primary\", size=\"lg\")\n",
        "            stats_output = gr.Textbox(label=\"Results\", lines=25, show_copy_button=True)\n",
        "            stats_btn.click(perform_descriptive_statistics, outputs=stats_output)\n",
        "\n",
        "        with gr.Tab(\"3ï¸âƒ£ ML Jobs\"):\n",
        "            gr.Markdown(\"### 4 Machine Learning Tasks\")\n",
        "            ml_btn = gr.Button(\" Run\", variant=\"primary\", size=\"lg\")\n",
        "            ml_output = gr.Textbox(label=\"Results\", lines=30, show_copy_button=True)\n",
        "            ml_btn.click(perform_ml_jobs, outputs=ml_output)\n",
        "\n",
        "        with gr.Tab(\"4ï¸âƒ£ Performance\"):\n",
        "            gr.Markdown(\"### Test on 1, 2, 4, 8 machines (Dynamic Scaling Model)\")\n",
        "            perf_btn = gr.Button(\" Run Performance Test\", variant=\"primary\", size=\"lg\")\n",
        "            perf_output = gr.Textbox(label=\"Performance Results\", lines=30, show_copy_button=True)\n",
        "            perf_btn.click(measure_performance_ui, outputs=perf_output)\n",
        "\n",
        "        with gr.Tab(\"5ï¸âƒ£ Results\"):\n",
        "            gr.Markdown(\"### Download from cloud\")\n",
        "            download_btn = gr.Button(\" View\", variant=\"primary\", size=\"lg\")\n",
        "            download_output = gr.Textbox(label=\"Files\", lines=20)\n",
        "            download_btn.click(download_results, outputs=download_output)\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    ---\n",
        "    **â˜ï¸ Storage:** Google Drive | ** Framework:** Apache Spark\n",
        "    \"\"\")\n",
        "\n",
        "demo.launch(share=True, debug=False)\n",
        "print(\"\\nâœ… Service running!\")"
      ],
      "metadata": {
        "id": "1BNVhX327SGF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df2448d-ca63-484d-f734-96e1a0073b9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-355517921.py:404: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(title=\"Cloud Data Processing Service\", theme=gr.themes.Soft()) as demo:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://65bd6065b0a3a0682b.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://65bd6065b0a3a0682b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Service running!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BwsMmipk8vfN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}